# -*- coding: utf-8 -*-
"""Audio Transciption.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lanEh0XsqVudSWT6Y1e51sGcTL4GLgH2

# **Testing with OpenAI Whisper Model**
This Colab notebook enables real-time audio recording through the browserâ€™s microphone and transcribes the captured speech into text using OpenAI's Whisper model. The setup leverages JavaScript for in-browser audio capture and Python (via the Whisper library) for speech recognition.

Of course, this is just a **simple testing** with the model, which eventually shows its robustness and can be used in various scenarios, especially in different backgrounds. Feel free to have a try with the functions below.ðŸ˜Š
"""

# Install whisper and dependencies
!pip install -q git+https://github.com/openai/whisper.git
!sudo apt update && sudo apt install ffmpeg -y

"""# Whisper Model
* Loads the base Whisper model (you can change this to tiny, small,
medium, or large).

* Feeds the saved recorded.wav into the model to generate a transcription.

* Displays the final transcribed text in the notebook.


Of course this is just a simple simulation of the code itself, in our implementation, the output will be just in text form immediately after transcribed and be proccesed by the AI assistant ðŸ˜Ž
"""

from IPython.display import display, Javascript
from google.colab import output
import base64
from pathlib import Path
import time
import whisper

# Load Whisper model once
model = whisper.load_model("medium")  # Options: tiny, base, small, medium, large

# JavaScript to record a short audio clip
RECORD_JS = """
const sleep = time => new Promise(resolve => setTimeout(resolve, time))
const b2text = blob => new Promise(resolve => {
  const reader = new FileReader();
  reader.onloadend = () => resolve(reader.result);
  reader.readAsDataURL(blob);
})

var record = async () => {
  const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
  const recorder = new MediaRecorder(stream);
  let chunks = [];

  recorder.ondataavailable = e => chunks.push(e.data);
  recorder.start();

  await sleep(5000);  // Record for 5 seconds
  recorder.stop();

  await new Promise(resolve => recorder.onstop = resolve);
  const blob = new Blob(chunks);
  const base64 = await b2text(blob);
  return base64;
};

record().then(b64 => {
  google.colab.kernel.invokeFunction('notebook.save_audio', [b64], {});
});
"""

# Python function to save audio and transcribe it
def save_audio(b64_audio):
    header, b64_data = b64_audio.split(',', 1)
    audio_bytes = base64.b64decode(b64_data)
    path = Path("recorded.wav")
    with open(path, "wb") as f:
        f.write(audio_bytes)
    print("âœ… Audio saved as 'recorded.wav'")

    # Transcribe using Whisper
    result = model.transcribe(str(path))
    print("\n--- Transcription ---")
    print(result["text"])

# Register callback
output.register_callback('notebook.save_audio', save_audio)

# Run the recording prompt
print("ðŸŽ¤ Please allow microphone access and speak...")
display(Javascript(RECORD_JS))
